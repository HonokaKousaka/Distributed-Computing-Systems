## 第 1 章 绪论

#### 1. 简述分布式系统的概念。

分布式系统是若干独立计算机的集合，这些计算机对于用户来说就像是一个单机的系统。

#### 2. 分布式系统可以分为哪些类型？

根据所依赖的硬件不同，分布式系统可以分为：

- 基于计算机构建的分布式系统

还可以进一步分为：

分布式计算系统（集群计算、网格计算、云计算）

分布式信息系统（事务处理，企业信息集成的信息管理系统）

- 基于微型设备构建的分布式系统（分布式普适系统）

智慧家庭、电子健康、传感网络

#### 3. 简述数据管理系统的发展历程。

第一代：层次、网状数据库系统

第二代：关系数据库系统（OLTP）

第三代：数据仓库系统（OLAP）

第四代：大数据管理系统（分布式、可扩展、高可用）

#### 4. 简述大数据的特征。

大数据的 5V：数量大、种类多、速度快、价值高、质量低。

#### 5. 数据库系统包含哪些主要的功能模块？

数据存储、数据组织模型与访问、查询处理和事务管理。

#### 6. 数据密集型应用与计算密集型应用有何区别？

计算密集型：计算机在处理这些应用时 CPU 处理能力是首要限制性因素。

数据密集型：计算机在处理这些应用时 I/O 带宽是首要限制性因素。

#### 7. 通用数据处理系统可以分为哪些类别？其分别适用于何种场景？

批处理系统：MapReduce, Spark(,Tez)

流计算系统(：Storm, Spark Streaming)

批流融合系统：Flink(, Structured Streaming)

批处理系统主要用于处理静态数据，适用于吞吐量要求较高而对实时性要求较低的场景。

流计算系统主要用于处理动态数据，适用于对实时性要求较高而对吞吐量要求较低的场景。

批流融合系统可用于上述两类场景，也适用于同一场景不同模块的数据特征和计算需求不同的场景。

#### 8. 分布式计算系统本身并无数据存储功能，则其输入数据从何而来，输出结果又走向何方？

分布式计算系统并非孤立地存在，而是与分布式存储系统、分布式资源管理系统和分布式协调服务系统等构成了庞大的系统生态圈。

输入数据来自分布式存储系统，输出结果也会进入分布式存储系统。



## 第 2 章 

#### 1. 试举例说明 HDFS 在实际应用中的作用。

HDFS 可以结合 MapReduce 解决超大规模的网页存储和分析处理问题，并且这是一个通用的处理技术。HDFS 的使用可以满足：

- 存储上百 GB/TB 级别的大文件
- 保证文件系统的容错
- 进行大文件的并发读写控制

#### 2. HDFS 的主要部件有哪些？各个部件分别有什么作用？

NameNode：负责 HDFS 的管理工作，包括管理文件目录结构、位置等元数据，维护 DataNode 的状态等，并不实际存储文件。

Secondary NameNode：充当 NameNode 的备份，当 NameNode 发生故障时利用 Secondary NameNode 进行恢复。

DataNode：负责存储文件，根据 NameNode 的控制信息存储和管理对应的文件块，并定期向 NameNode 汇报自身的状态。

#### 3. HDFS 的文件分块是什么？和操作系统的文件系统中块的概念有何联系与区别？

HDFS 将大文件切分为若干个固定大小的文件块，并将其存储于 DataNode中，作为 HDFS 读写文件的最小单位。

操作系统中的文件系统通常会将文件分成若干块，按块在磁盘中进行存储，文件系统输入输出的最小单位为磁盘块。操作系统的文件系统与 HDFS 在本质上都是文件系统，因此他们的分块都是为了更好地管理文件，非常相似。

但是，HDFS 为了加快文件访问速度以及避免分布式环境下宕机带来文件块丢失的问题，HDFS 对这些文件块会进行备份。

#### 4. 简述客户端从 HDFS 中读取某一文件时的工作过程。

1. 客户端与 NameNode 通信，请求读取一个文件。
2. NameNode 根据文件的路径等信息判断读取请求是否合法，如果合法则向客户端返回文件中所有数据块的存放地址。
3. 对于第 1 个数据块，客户端从距离最近的存放该数据块的 DataNode 读取数据。
4. 当第 1 个数据块读取完毕后，客户端从距离最近的存放第 2 个数据块的 DataNode 读取数据。
5. 以此类推，客户端读取下一个数据块，直到读取完毕所有的数据块。

#### 5. HDFS 为什么要采用“一次写入，多次读取”的方式？

“一次写入”是因为希望用户在编程时无须考虑文件的并发读写问题，避免了读写不一致的情况，提升了数据写入与读取的一致性。

如果允许文件在写入后进行内容更新，那么元数据会发生改变，且存放在多个 DataNode 中的副本也需要改变以确保一致性，数据同步的过程非常复杂。

读操作可以并发执行，提升了读文件的效率，而且也不会破坏文件一致性。

#### 6. 为什么 HDFS 要按照逐个进行的阻塞方式读写文件块？能否并行读写文件块？

只可以按照逐个进行的阻塞方式读写文件块。

在写的时候，由于 HDFS 中节点的存活状态（随时都可能的故障）、负载均衡等情况在文件块写入过程中会发生变化，故 NameNode 无法预先决定所有文件块的存放位置，因此文件块之间只能以阻塞的方式传输。

在读的时候，因为需要顺序读，故需要阻塞。

**可以并行的是对一个文件整体进行读操作，对文件块则不可以并行读。**



## 第 3 章

#### 1. MapReduce 与 MPI 相比所具备的优势是什么？

从用户编程的角度来看，程序员在编写 MapReduce 代码时不需要考虑进程之间的并行问题，并且进程之间的通信不需要用户在程序中显式地表达。

从系统实现的角度来看，MapReduce 具有容错能力，减少了计算资源的浪费。

#### 2. 请解释 MapReduce 的逻辑计算模型和物理计算模型。

MapReduce 的逻辑计算模型如下图，仅包含 map 和 reduce 两个算子。是一个只包含两个顶点的有向无环图。

![image-20240606194402809](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20240606194402809.png)

MapReduce 的物理计算模型如下图。为了支持大规模的并行处理，两个算子在物理上需要由若干实例实现，这里的实例是指进程或线程。

![image-20240606194607764](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20240606194607764.png)

#### 3. MapReduce 的主要部件有哪些？各个部件分别有什么作用？

主要部件有 JobTracker, TaskTracker, Task以及客户端。

JobTracker：主节点运行的后台进程，负责整个系统的资源管理和作业管理。资源管理主要是指JobTracker通过监控TaskTracker管理系统拥有的计算资源，作业管理是指JobTracker负责将作业拆分成任务，并进行任务调度以及跟踪任务的运行进度、资源使用量等信息。

TaskTracker：从节点运行的后台进程，负责管理本节点的资源、执行JobTracker的命令并汇报情况。使用slot等量划分本节点中的资源量（CPU，内存等），接受JobTracker发送的命令并执行（如启动新任务、杀死任务等），通过心跳将本节点的资源使用情况和任务运行进度汇报给JobTracker。

Task：从节点在应用程序执行过程中启动的进程，负责任务执行。JobTracker根据TaskTracker汇报的信息进行调度，命令存在空闲slot的TaskTracker启动Task进行执行Map或Reduce任务，即MapTask或ReduceTask。在Hadoop MapReduce的架构中，该进程的名称是Child。

客户端：客户端所在节点为提交应用程序启动的进程，负责将用户编写的MapReduce程序提交给JobTracker。在Hadoop MapReduce的架构中，该进程的名字是RunJar。

![image-20240613174926634](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20240613174926634.png)

#### 4. 如果不考虑数据的输入和输出阶段，MapReduce 的工作过程可以划分为哪些阶段？  各个阶段主要完成哪些功能？

可以划分为Map，Shuffle和Reduce阶段。

Map阶段：对一个键值对来说，Map过程将其转换为一个或多个键值对，即将[K1, V1]转换为List([K2, V2])。

Shuffle阶段：对于一组键值对来说，Shuffle过程将键相同的键值对发送给同一个Reduce任务，即将List([K2, V2])键值对转换为[K2, List(V2)]。

Reduce阶段：对一个键值对来说，Reduce过程将其转换为一个或多个键值对，即将[K2, List(V2)]键值对转换为[K3, V3].

#### 5. 请简述 MapReduce 与 HDFS 之间的关系。

MapReduce是Hadoop项目下的一个分布式计算系统，HDFS是Hadoop项目下的一个分布式文件系统。

他们的共同之处在于，他们都本属于Hadoop项目。

他们的区别在于，他们二者并不是强行绑定的，相反，二者可以认为是两个不同的项目。在Spark或Flink中，我们也可能使用HDFS作为分布式文件系统，而在MapReduce中我们也不一定就只是使用HDFS作为分布式文件系统。

#### 6. MapReduce 中的归并（merge）与合并（combine）有什么区别？

归并是在Map阶段与Reduce阶段中，溢写进入磁盘的文件数量达到阈值后，通过归并操作形成一个大文件。

合并是在Map阶段，可以将键相同的键值对进行合并，此法可以降低Map和Reduce任务消耗的存储空间，减少Reduce阶段需要处理的数据量使程序性能得到提升，更重要的是有利于降低Shuffle过程中传输的数据量。

#### 7. MapReduce中的分布式缓存机制有什么作用？

在一些情况下，存在一些数据量较小但是被大部分任务需要的数据，在Map完成后进行Shuffle时会导致非常大的I/O代价。此时，我们可以将这些数据量较小的数据进行分布式缓存，从而让Map阶段可以实现更多功能，可能可以避免Shuffle的大量I/O代价，提高程序的运行效率。

#### 8. 如果 MapReduce 以 HDFS 中的文件作为输入，那么 InputFormat 中的 split 与 HDFS 中的 block 是否必然一一对应？为什么？

不是。HDFS存储文件的方式是将文件以block的方式存储，每个block的大小相同，但是这导致在逻辑上有可能某个键值对被分在两个物理块中，导致了跨块的逻辑记录。为了让Map任务读取的输入数据完整，InputFormat可以将数据在逻辑上划分为若干分片，键值对不会被分在两个物理块中，Map任务读取的输入数据不会存在缺失。

#### 9. 如果 MapReduce 运行过程中仅有一个 Reduce 任务的进程崩溃而其他部件正常，那么 MapReduce 系统会重启一个 Reduce 任务。请问重启后的 Reduce 任务从哪里获取输入数据？

由于Reduce任务的输入数据均来自于之前完成的Map数据输出于本地磁盘的数据，因此只要磁盘数据不丢失，Reduce任务即可读取相应数据。如果无法读取到Map端的文件，那么Map端也需要重启，从而重新从磁盘中得到Map端的输出数据。

#### 10. 假设 Map 任务的缓冲区“无限大”，则是否仍有必要溢写磁盘？类似地，如果 Reduce 任务的缓冲区“无限大”，则是否仍需溢写磁盘？为什么？

**助教观点**：Map需要溢写，因为需要基于写到磁盘来实现Reduce的容错。Reduce可以不要溢写，因为可以让所有操作都在内存中完成，大大减小了I/O开销。



## 第 4 章

#### 1. 与 Spark 相比，MapReduce 存在哪些局限性？

1. 编程框架的表达能力有限，用户编程复杂。MapReduce只提供了map和reduce两个编程算子，用户需要基于二者实现数据处理的操作。一些常用操作需要利用map和reduce才能实现，增加了编程的复杂度。
2. 单个作业中需要进行Shuffle的数据以阻塞方式传输，磁盘I/O开销大、延迟高。在单个作业中，需要进行Shuffle的数据首先由Map任务将计算结果写入本地磁盘，之后Reduce任务才可读取该计算结果。因此，需要进行Shuffle的数据磁盘I/O开销大，同时这种阻塞式数据传输方式加剧了MapReduce作业的高延迟。
3. 多个作业之间的衔接涉及I/O开销，应用程序的延迟高。对于单个MapReduce作业，这是从HDFS获得输入再往HDFS输出的过程。然而，很多应用程序需要通过多个作业完成，例如机器学习的迭代训练过程。因此，在迭代计算过程中，前一个作业需要将迭代计算的结果写入HDFS或其他存储系统，后一个作业从HDFS或其他存储系统中读取该结果并进行新一轮迭代计算。迭代计算中间结果的反复读写导致整个应用的延迟非常高。
4. 另外，第一代MapReduce资源与作业管理未被分离，耦合度较高。

#### 2. 请说明 Spark 的逻辑计算模型和物理计算模型之间的关系。

逻辑计算模型中，Operator DAG从算子的角度描述计算的过程。RDD Lineage从数据（RDD变换）的角度来描述计算过程。逻辑执行图描绘了算子之间的数据流动关系，物理执行图是从逻辑执行图过来的，逻辑执行图上的一个算子在物理执行图上会有多个并行实例。

#### 3. 什么是 RDD Lineage? 当 Lineage 较长时，如何加快故障恢复？

RDD Lineage是Spark的一种逻辑计算模型，其描述的主体是数据。在RDD Lineage中，通过读入外部数据源进行RDD创建，经过一系列的转换操作，每次均会产生不同的RDD，以供下一个转换操作使用，最后一个RDD经过“行动”操作进行转换，并输出到外部数据源。

当Lineage较长时，我们需要使用检查点机制。检查点机制将RDD写入可靠的外部分布式文件系统，例如HDFS。如果用户指定某些RDD需要设置检查点，则系统将在作业结束之后启动一个独立的作业进行写检查点操作。当Lineage较长或宽依赖过多时，如果对中间某个RDD进行写检查点操作，则之后发生节点故障时，从生成检查点的RDD开始按照Lineage重新执行计算，能够加快恢复过程。

#### 4. 请简述 Standalone Client 与 Standalone Cluster 两种模式下 Spark 架构之间的区别， 并画出两种模式下的架构图。

Standalone Client模式是指以Standalone方式部署Spark系统，客户端选择以Client方式提交应用程序。Standalone Client模式下的驱动器位于客户端，并处于SparkSubmit进程中。此时，客户端可以查看应用程序执行过程中的信息。

Standalone Cluster模式是指以Standalone方式部署Spark系统，客户端选择以Cluster方式提交应用程序。在Standalone Cluster模式下，Master将选择在某一Worker所在节点启动名为DriverWrapper的进程作为驱动器。此时，客户端无法查看应用程序执行过程中的信息。

#### 5. Spark 如何划分 DAG 中的 Stage?

DAG调度器通过分析各个RDD中分区之间的依赖关系决定如何划分Stage。简单来说，DAG调度器针对DAG进行反向解析，遇到宽依赖则生成新的Stage，遇到窄依赖则将该Operator加入当前Stage，从而使窄依赖尽可能划分在同一个Stage里。

#### 6. Spark 中的应用和作业存在何种关系？

从逻辑执行角度来看，一个Application由一个或多个DAG构成，一个DAG由一个或多个Stage构成，一个Stage由若干RDD Operator构成。

从物理执行角度来看，一个Application等于一个或多个Job，一个Job由一个或多个TaskSet构成，一个TaskSet为多个不存在Shuffle关系的Task集合。

#### 7. Spark 中的 Stage 和 Task 存在怎样的联系？

从物理执行角度来看，一个Application等于一个或多个Job，一个Job由一个或多个TaskSet构成，一个TaskSet为多个不存在Shuffle关系的Task集合。Stage是Job的基本调度单位，代表了一组关联的，相互之间没有Shuffle依赖关系的Task组成的TaskSet。

#### 8. Spark 中 Stage 内部如何进行数据传输？Stage 之间如何进行数据传输？

Spark系统采用流水线（pipeline）方式进行Stage内部的数据传输。流水线方式不要求物化前序算子的所有计算结果。在Stage内部，无须采用Shuffle方式进行数据传输。

为了便于描述Stage之间的数据传输，此处将Shuffle过程分为Shuffle Write和Shuffle Read两个阶段。在Shuffle Write阶段，ShuffleMapTask需要将输出RDD的记录按照分区函数划分到相应的bucket中并物化到本地磁盘形成ShuffleblockFile，之后才可以在Shuffle Read阶段被拉取。在Shuffle Read阶段，ShuffleMapTask或ResultTask根据partition函数读取相应的ShuffleblockFile，将其存入缓冲区并继续进行后续的计算。此处Buffer并不一定要求对该Task负责处理的所有记录进行排序后才进行后续处理。

#### 9. Spark 中的 RDD 持久化和检查点机制存在哪些异同点？

相同之处：他们都为Spark提供了容错。

不同之处：

RDD持久化是将某个RDD存储在内部的一个或多个节点上，但检查点机制是将RDD写入外部的分布式文件系统，如HDFS。

RDD持久化可以通过缓存RDD加快计算速度，也可以利用多个备份快速恢复因故障丢失的数据分区。但检查点机制主要是为了加快故障恢复速度。

RDD持久化可以在计算过程中发生，而检查点机制是在一个独立的作业结束之后进行。

#### 10. Spark 中的广播变量机制通常用于什么场景？

在一些情况下，存在一些数据量较小但是被大部分任务需要的数据，Shuffle时会导致非常大的I/O代价。Spark 的广播变量机制可以允许我们将较小的数据作为广播变量进行广播，可能可以避免Shuffle的大量I/O代价，提高程序的运行效率。



## 第 5 章 资源管理系统 Yarn

#### 1. 对于 MapReduce 和 Spark 而言，应用与作业是否存在区别？

对于MapReduce来说，是没有区别的。

对Spark来说，一个应用可能包含一个或多个作业。

#### 2. Spark 架构与 Yarn 架构在设计思想上有无共同点？

有共同点。两者都是主从架构，并且都是将作业管理与资源管理相分离，使作业之间相互独立地控制执行。

#### 3. 第二代 Hadoop 与第一代 Hadoop 相比所具备的优势是什么？

第一代Hadoop中的MapReduce存在一定局限性。从系统架构的角度来看，第一代MapReduce存在**资源管理与作业紧密耦合**，由JobTracker负责作业管理和任务调度，以及管理集群中的资源。另外，JobTracker负责系统中所有作业的控制，JobTracker需要维护所有作业的元信息，内存开销较大，当同一时刻执行的作业数量增加时，JobTracker与执行这些作业的任务以及TaskTracker之间的通信频率增大，造成JobTracker进程是不稳定，造成**作业的控制管理高度集中**的问题。

第二代Hadoop中的MapReduce采用了Yarn作为独立出来的资源管理系统，MapReduce 2.0作为计算系统负责作业管理。相比于第一代Hadoop中，JobTracker与TaskTracker都会负责资源管理与作业管理，MapReduce 2.0让负责资源管理的进程只有RM和NM，作业管理进程则是AM。Yarn将资源管理系统从第一代MapReduce系统中独立，通过将资源管理与作业分离形成通用的资源管理系统，并使作业之间相互独立地控制执行。

#### 4. Yarn 主要包含哪些部件？各个部件分别有什么作用？

ResourceManager（RM）：负责整个系统的资源管理和分配的资源管理器，主要由调度器和应用管理器两个组件构成。其中，调度器负责分配Container并进行资源调度，应用管理器负责管理整个系统中运行的所有应用，包括应用程序提交、与调度器协商资源以启动ApplicationMaster，监控ApplicationMaster的运行状态等。

NodeManager（NM）：负责每个节点资源和任务管理的节点管理器。一方面，NM定时向RM汇报本节点的资源使用情况和Container运行状态；另一方面，NM接收并处理来自AM的Container启动/停止等各种请求。

ApplicationMaster（AM）：每当用户基于Yarn平台提交一个框架应用，Yarn均启动一个AM以管理该应用。一方面，AM与RM调度器通过协商获取资源（以Container表示），将获取的资源进一步分配给作业内部的任务；另一方面，AM与NM通过通信启动/停止任务，监控所有任务运行状态，并在任务发生故障时重新申请资源以重启任务。

Container：Container为资源的抽象表示，包含CPU、内存等资源，是一个动态资源划分单位。当AM向RM申请资源时，RM向AM返回以Container表示的资源。

#### 5. Yarn 中 Container 内的任务由谁负责启动/停止？

AM与NM通过通信启动/停止任务。所以，AM负责Container的启动与停止，而NM是真正启动与停止任务的进程。

#### 6. 简要分析 FIFO、Capacity 和Fair 3种调度器的优缺点。

FIFO：实现简单，但导致一个应用独占所有资源而其他应用需要不断等待，使得平均每个应用等待时间过长。

Capacity：避免了某个长时间运行的应用独占集群资源而其他应用得不到运行的情况，但可能会有时候让某个队列的资源处于空闲状态，造成集群资源的浪费。

Fair：允许队列间共享资源，避免了浪费。但在实际执行中，会发生资源的抢占，因此实现上必须保证资源的分配与抢占不会出现问题，实现可能较为复杂。

#### 7. 简述 ApplicationMaster 申请资源的过程。

1. AM向RM注册，以便客户端通过RM查看应用程序的资源使用情况。AM将应用解析为作业，并进一步分解为若干任务，并向RM申请启动这些任务的资源。
2. RM向提出申请的AM分配以Container形式表示的资源。一旦AM申请到资源，即可在多个任务间进行资源分配。
3. AM确定资源分配方案后，与对应的NM通信，在对应的Container中启动相应的工作进程以执行任务。

#### 8. NodeManager 是否监控 Container 中任务的运行情况？

NM只会在意自己节点上的资源使用情况与Container的运行情况，不会在意其中任务的运行情况。AM会监控Container中任务的运行情况。

#### 9. ApplicationMaster 由谁监控？并简述其容错恢复过程。

由ResourceManager监控。AM以及Container中运行的任务与具体框架有关，因此作为资源管理平台的Yarn在该类任务发生故障时只会帮助其重新启动，而运行过程需要由框架完成恢复。

#### 10. 为什么引入 Yarn 之后，MapReduce 无法独立运行？

引入Yarn后，JobTracker和TaskTracker消失了，资源管理功能依赖于RM和NM，作业管理功能依赖于MRAppMaster。MapReduce不可能对其本身进行资源管理和作业管理，也就不可能独立运行。

#### 11. Yarn 平台运行 MapReduce 的方式与 Spark 的 Yarn Cluster 和 Yarn Client 两种模式中的哪一种更相似？为什么？

与Yarn Cluster更相似。因为Yarn Cluster中的Application Master在一个从节点中，且不但负责根据Yarn分配的资源进行任务调度，也会启动CoarseGrainedExecutorBackend进程执行任务。Yarn平台运行MapReduce时，MRAppMaster位于从节点中，且也会负责任务调度与应用管理，这与Yarn Cluster相近。而Yarn Client则将Driver置于客户端内，这与MapReduce不同。

#### 12. 试画出 Yarn 平台同时运行 MapReduce 和 Spark 的架构图，其中 Spark 应用程序以 Yarn Client 模式执行。

![image-20240620210555237](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20240620210555237.png)

这张图的SparkSubmit多加一个（Driver），ApplicationMaster改为ExecutorLauncher。



## 第 10 章 批流融合系统 Flink

#### 1. Flink 使用何种数据模型表示数据？其与 MapReduce、Spark 和 Storm 等有何区别？

Flink采用DataStream作为数据模型，这是一个不间断的，无界的记录序列。

MapReduce以键值对作为数据模型，Spark系统以RDD作为数据模型。

MapReduce将数据抽象到记录的级别，Spark和Flink都是将数据抽象到记录集合的级别。

MapReduce和Spark的数据模型认为输入数据的有界的，Flink系统的数据模型则认为输入数据是无界的。

#### 2. 在 Standalone 模式下，Flink 的架构包含哪些部件？各个部件分别有何功能?

Client：客户端，将用户编写的DataStream程序翻译为逻辑执行图并进行优化，并将优化后的逻辑执行图提交到JobManager，系统运行时Client的进程名为CliFrontend。

JobManager：作业管理器，根据逻辑执行图生成物理执行图，负责协调系统的作业执行，包括任务调度、协调检查点和故障恢复等。在Standalone模式下，JobManager同样负责Flink系统的资源管理，系统运行时的进程名为StandaloneSessionClusterEntrypoint。

TaskManager：任务管理器，用于执行JobManager分配的任务，并且负责读取数据、缓存数据以及与其他TaskManager进行数据传输。在Standalone模式下同样负责所在节点的资源管理，将内存等抽象资源为若干TaskSlot并用于任务执行，系统运行时的进程名为TaskManagerRunner。

#### 3. 在 Standalone 模式下，Flink 中的 JobManager 和 TaskManager 与 Spark 中的 Master 和Worker 在功能上是否完全一致？

并不一致。Spark的Master和Worker只负责资源管理，但Flink的JobManager和TaskManager却在Standalone模式下也负责作业管理。

#### 4. 在 Standalone 模式下，同一个 TaskManager 可能同时执行不同应用程序的任务，某种程度上会存在应用程序之间的相互干扰，引入Yarn后是如何解决该问题的？

相互干扰本质上是因为资源与作业管理并未分离。引入Yarn以后，分离了作业管理与资源管理，因此不同Flink作业 / 应用的执行互不干扰。

#### 5. Flink 中的逻辑执行图与物理执行图分别如何产生？与 Spark 有何区别？

Client根据DataStream程序生成逻辑执行图（DAG），并进一步分析各个算子之间的数据依赖关系。借用Spark中描述RDD依赖关系时的宽依赖和窄依赖的概念，如果算子之间的数据依赖为窄依赖关系，算子之间呈现一对一的数据传递关系。如果算子由处于不同TaskManager的任务实现，则会带来不同TaskManager之间的数据传输。为了避免算子由处于不同TaskManager的任务实现，Flink使用Chaining机制进行优化，将部分算子合并为一个“大”算子。Chaining优化并不改变算子的语义，但可以避免数据在不同TaskManager之间的非必要传输。

JobManager收到Client提交的逻辑执行图后，根据算子的并行度，将逻辑执行图转换为物理执行图。

Spark中，驱动器创建的SparkContext根据RDD的依赖关系构建DAG（逻辑执行图），由DAG调度器划分DAG Stage，将一个Stage作为一个TaskSet提交给任务调度器（物理执行图），由其将任务分发给各个工作节点上的执行器执行，整个过程按照Stage依次执行。

区别是，Spark中的驱动器不会对算子进行合并，并且是在驱动器上生成逻辑执行图与物理执行图，但Flink的逻辑执行图在客户端生成，JobManager生成物理执行图。

#### 6. Flink 中的非迭代任务间如何进行数据传输？与 Spark 有何区别？

采用流水线机制进行数据传输，一次传输一个缓冲区。Spark的数据传输是阻塞式的，会将数据写到磁盘中，Flink的数据传输是非阻塞的，只是将数据存入内存缓冲区。

#### 7. 简述异步屏障快照的工作过程。

JobManager在输入记录中插入屏障，这些屏障与记录一起向下游计算任务流动，每个屏障指示检查点的ID。实现算子的任务收到来自上游任务中所有标识为$n$的屏障（即屏障对齐）后保存其状态，随着记录的流动，所有算子保存的状态形成检查点$n$。值得指出的是，某一任务将标识为$n$的屏障对齐后，能够继续接收属于检查点$n+1$的记录。因此，同一时刻系统中的不同任务保存属于不同检查点的状态。

#### 8. Flink 中的迭代算子如何实现数据反馈？

利用迭代前端和迭代后端两类特殊的任务实现数据反馈，两类任务成对处于同一个TaskManager，迭代末端任务的输出可以再次作为迭代前端任务的输入。在流式迭代计算中，通常每轮迭代计算的部分结果作为输出向后传递，而另一部分结果作为下一轮迭代计算的输入，并且迭代过程将持续进行。

#### 9. 如果 Flink 系统不进行状态管理，将造成哪些缺陷？

Flink的流水线传输机制决定了DataStream的“不完整”和不断变化，这些特点让它无法像Spark一样支持DataStream的持久化，自然也无法像Spark那样利用RDD Lineage进行故障恢复。如果不进行状态管理，一旦出现故障系统需要从头开始计算，对于用户来说是无法接受的。

#### 10. MemoryStateBackend 和 FsStateBackend 两种状态存储方式中，哪一种更适用于实际生产环境？为什么？

FsStateBackend更适合。因为MemoryStateBackend在写检查点时将状态发送给JobManager的内存，但JobManager也可能故障，且内存也非常有限。而FsStateBackend写检查点时会将状态发送给HDFS，容量更大，且相对更加可靠。

#### 11. Flink 为何无法像 Spark 那样利用 Lineage 进行故障恢复？

Spark中的RDD是静态的数据，Flink的流水线传输机制决定了DataStream的“不完整”和不断变化。而这些特点让它无法像Spark一样支持DataStream的持久化，自然也无法像Spark那样利用RDD Lineage进行故障恢复。
